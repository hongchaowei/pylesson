<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3课: 深度学习与金融时间序列 - Python金融编程课程</title>
    <link rel="stylesheet" href="../../css/style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>

    <style>
    /* 确保代码块左对齐 */
    .lesson-content pre {
        text-align: left !important;
        font-family: 'Courier New', Courier, monospace;
        background-color: #f5f7fa;
        padding: 15px;
        border-radius: 8px;
        overflow-x: auto;
        margin: 20px 0;
        border-left: 4px solid #4299e1;
    }
    
    .lesson-content code {
        text-align: left !important;
        font-family: 'Courier New', Courier, monospace;
        background-color: #f5f7fa;
        padding: 2px 5px;
        border-radius: 3px;
    }
    
    /* 确保pre标签内的内容左对齐 */
    pre * {
        text-align: left !important;
    }
    
    /* 确保所有代码相关元素都左对齐 */
    pre, code, .hljs {
        text-align: left !important;
    }
    </style>
    
</head>
<body>
<div id="navigation-container"></div>
<script>
// 动态加载导航条
function loadNavigation() {
    fetch('../../nav.html')
        .then(response => response.text())
        .then(html => {
            // 更新导航中的链接路径
            let updatedHtml = html;
            
            // 处理根目录文件链接（index.html, syllabus.html等）
            updatedHtml = updatedHtml.replace(/href="index\.html"/g, 'href="../../index.html"');
            updatedHtml = updatedHtml.replace(/href="\.\/([^/]*\.html)"/g, 'href="../../$1"');
            
            // 处理模块路径链接（已经是完整相对路径，只需要添加根路径前缀）
            updatedHtml = updatedHtml.replace(/href="\.\/part([^"]*)"/g, 'href="../../part$1"');
            
            document.getElementById('navigation-container').innerHTML = updatedHtml;
            
            // 添加移动端菜单切换功能
            const menuToggle = document.getElementById('menuToggle');
            const navMenu = document.querySelector('.nav-menu');
            if (menuToggle && navMenu) {
                menuToggle.addEventListener('click', function() {
                    navMenu.classList.toggle('active');
                });
            }
        })
        .catch(error => console.error('导航加载失败:', error));
}

// 页面加载完成后加载导航
document.addEventListener('DOMContentLoaded', loadNavigation);
</script>



<main class="container">
        <section class="lesson-content">
            <h2>循环神经网络(RNN/LSTM)</h2>
            <p>RNN和LSTM网络特别适合处理金融时间序列数据。</p>
            <h3>1. LSTM价格预测</h3>
            <pre><code class="language-python">import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
# 准备数据
data = pd.read_csv('stock_prices.csv', parse_dates=['date'])
prices = data['close'].values.reshape(-1, 1)
# 数据标准化
scaler = MinMaxScaler()
scaled_prices = scaler.fit_transform(prices)
# 创建时间序列数据集
def create_dataset(data, time_step=60):
    X, y = [], []
    for i in range(len(data)-time_step-1):
        X.append(data[i:(i+time_step), 0])
        y.append(data[i+time_step, 0])
    return np.array(X), np.array(y)
X, y = create_dataset(scaled_prices)
X = X.reshape(X.shape[0], X.shape[1], 1)
# 构建LSTM模型
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
# 训练模型
model.fit(X, y, epochs=20, batch_size=32)</code></pre>
            <h3>2. 多变量LSTM</h3>
            <pre><code class="language-python"># 使用多变量数据
features = data[['open', 'high', 'low', 'volume', 'close']]
scaled_features = scaler.fit_transform(features)
# 创建多变量数据集
def create_multivariate_dataset(data, time_step=60):
    X, y = [], []
    for i in range(len(data)-time_step-1):
        X.append(data[i:(i+time_step), :])
        y.append(data[i+time_step, -1])  # 预测收盘价
    return np.array(X), np.array(y)
X_multi, y_multi = create_multivariate_dataset(scaled_features)
# 构建多变量LSTM模型
model = Sequential()
model.add(LSTM(100, return_sequences=True, input_shape=(X_multi.shape[1], X_multi.shape[2])))
model.add(LSTM(100))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_multi, y_multi, epochs=30, batch_size=32)</code></pre>
        </section>
        <section class="lesson-content">
            <h2>卷积神经网络(CNN)</h2>
            <p>CNN可以捕捉金融时间序列中的局部模式和趋势。</p>
            <h3>1. 1D CNN价格预测</h3>
            <pre><code class="language-python">from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten
# 构建1D CNN模型
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X, y, epochs=20, batch_size=32)</code></pre>
            <h3>2. CNN-LSTM混合模型</h3>
            <pre><code class="language-python">from tensorflow.keras.layers import TimeDistributed
# 重塑数据以适应CNN-LSTM
X_reshaped = X.reshape(X.shape[0], X.shape[1], 1, 1)
# 构建CNN-LSTM模型
model = Sequential()
model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'),
                         input_shape=(None, X.shape[1], 1)))
model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
model.add(TimeDistributed(Flatten()))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_reshaped, y, epochs=20, batch_size=32)</code></pre>
        </section>
        <section class="lesson-content">
            <h2>注意力机制</h2>
            <p>注意力机制可以帮助模型关注金融时间序列中的关键时间点。</p>
            <h3>1. 注意力LSTM</h3>
            <pre><code class="language-python">from tensorflow.keras.layers import Multiply, Permute, RepeatVector, Lambda, Dot
import tensorflow.keras.backend as K
# 注意力机制实现
def attention_block(inputs):
    input_dim = int(inputs.shape[2])
    attention = Dense(1, activation='tanh')(inputs)
    attention = Flatten()(attention)
    attention = Activation('softmax')(attention)
    attention = RepeatVector(input_dim)(attention)
    attention = Permute([2, 1])(attention)
    return Multiply()([inputs, attention])
# 构建带注意力的LSTM模型
inputs = Input(shape=(X.shape[1], 1))
lstm_out = LSTM(50, return_sequences=True)(inputs)
attention_out = attention_block(lstm_out)
attention_out = LSTM(50)(attention_out)
outputs = Dense(1)(attention_out)
model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='mean_squared_error')</code></pre>
            <h3>2. Transformer模型</h3>
            <pre><code class="language-python">from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout
class TransformerEncoder(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super().__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)
    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)
# 构建Transformer模型
inputs = Input(shape=(X.shape[1], 1))
x = Dense(64)(inputs)
x = TransformerEncoder(64, 4, 128)(x)
x = GlobalAveragePooling1D()(x)
outputs = Dense(1)(x)
model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='mean_squared_error')</code></pre>
        </section>
        <section class="lesson-content">
            <h2>多模态金融数据</h2>
            <p>结合多种数据源可以提高金融预测的准确性。</p>
            <h3>1. 新闻情感分析</h3>
            <pre><code class="language-python">from transformers import BertTokenizer, TFBertModel
import tensorflow as tf
# 加载预训练BERT模型
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
# 处理新闻文本
news_texts = ["Apple stock rises after positive earnings report",
              "Market volatility increases due to geopolitical tensions"]
inputs = tokenizer(news_texts, return_tensors='tf', padding=True, truncation=True)
outputs = bert_model(inputs)
news_embeddings = outputs.last_hidden_state[:, 0, :]  # 取[CLS]标记的嵌入</code></pre>
            <h3>2. 多模态融合模型</h3>
            <pre><code class="language-python"># 时间序列输入
ts_input = Input(shape=(X.shape[1], 1))
lstm_out = LSTM(64)(ts_input)
# 新闻输入
news_input = Input(shape=(768,))  # BERT嵌入维度
dense_news = Dense(32)(news_input)
# 融合层
concatenated = Concatenate()([lstm_out, dense_news])
dense1 = Dense(64, activation='relu')(concatenated)
output = Dense(1)(dense1)
model = Model(inputs=[ts_input, news_input], outputs=output)
model.compile(optimizer='adam', loss='mean_squared_error')</code></pre>
        </section>
        <div class="lesson-navigation">
            <a href="lesson2.html" class="btn">« 上一课: 无监督学习在金融中的应用</a>
            <a href="lesson4.html" class="btn">下一课: 模型解释与可解释性 »</a>
        </div>
    </main>
    <footer>
        <div class="container">
            <p>© 2023 Python金融编程课程. 保留所有权利.</p>
        </div>
    </footer>
</body>
</html>
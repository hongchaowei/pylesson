#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”¨æˆ·æä¾›çš„èšç±»åˆ†æä»£ç ç¤ºä¾‹
ç°åœ¨å¯ä»¥æ­£å¸¸è¿è¡Œï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»ç”Ÿæˆäº†æ‰€éœ€çš„æ•°æ®æ–‡ä»¶
"""

import pandas as pd 
from sklearn.cluster import KMeans 
from sklearn.preprocessing import StandardScaler 

print("è¿è¡Œç”¨æˆ·æä¾›çš„èšç±»åˆ†æä»£ç ç¤ºä¾‹")
print("=" * 50)

# åŠ è½½è‚¡ç¥¨æ”¶ç›Šæ•°æ® 
stocks = pd.read_csv('stock_returns.csv', index_col='date') 
print(f"âœ“ æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {stocks.shape}")
print(f"âœ“ è‚¡ç¥¨åˆ—è¡¨: {list(stocks.columns)}")

# æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
print("\næ•°æ®é¢„è§ˆ:")
print(stocks.head())

# å¤„ç†ç¼ºå¤±å€¼
stocks = stocks.dropna()
print(f"\næ¸…ç†åæ•°æ®å½¢çŠ¶: {stocks.shape}")

# æ•°æ®æ ‡å‡†åŒ– 
scaler = StandardScaler() 
scaled_data = scaler.fit_transform(stocks) 
print(f"âœ“ æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")

# èšç±»åˆ†æ 
kmeans = KMeans(n_clusters=5, random_state=42) 
clusters = kmeans.fit_predict(scaled_data) 
print(f"âœ“ K-meansèšç±»å®Œæˆï¼Œå‘ç° {len(set(clusters))} ä¸ªèšç±»")

# åˆ†æèšç±»ç»“æœ 
stocks['cluster'] = clusters 
cluster_stats = stocks.groupby('cluster').mean()

print("\nèšç±»ç»Ÿè®¡ç»“æœ:")
print(cluster_stats)

print("\nå„èšç±»çš„æ ·æœ¬æ•°é‡:")
print(stocks['cluster'].value_counts().sort_index())

print("\nâœ… ç”¨æˆ·ä»£ç è¿è¡ŒæˆåŠŸï¼")
print("\nğŸ’¡ è¯´æ˜:")
print("   - ç°åœ¨æ‚¨çš„ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œäº†")
print("   - æˆ‘ä»¬ç”Ÿæˆäº†åŒ…å«8åªè‚¡ç¥¨çº¦2å¹´å†å²æ•°æ®çš„è™šæ‹Ÿæ•°æ®é›†")
print("   - æ•°æ®åŒ…æ‹¬AAPL, MSFT, GOOG, AMZN, JPM, BAC, TSLA, NVDA")
print("   - æ‚¨å¯ä»¥ä¿®æ”¹èšç±»æ•°é‡æˆ–å°è¯•å…¶ä»–å‚æ•°")
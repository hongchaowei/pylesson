#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ç”¨æˆ·æä¾›çš„èšç±»åˆ†æä»£ç ç¤ºä¾‹
"""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

def test_clustering_analysis():
    """æµ‹è¯•èšç±»åˆ†æä»£ç """
    print("æµ‹è¯•è‚¡ç¥¨æ”¶ç›Šæ•°æ®èšç±»åˆ†æ")
    print("=" * 50)
    
    try:
        # åŠ è½½è‚¡ç¥¨æ”¶ç›Šæ•°æ®
        stocks = pd.read_csv('stock_returns.csv', index_col='date')
        print(f"âœ“ æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {stocks.shape}")
        print(f"âœ“ æ•°æ®æ—¥æœŸèŒƒå›´: {stocks.index.min()} åˆ° {stocks.index.max()}")
        print(f"âœ“ è‚¡ç¥¨åˆ—è¡¨: {list(stocks.columns)}")
        
        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        print("\næ•°æ®é¢„è§ˆ:")
        print(stocks.head())
        
        print("\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
        print(stocks.describe())
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_values = stocks.isnull().sum()
        print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡:")
        print(missing_values)
        
        # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
        stocks_clean = stocks.dropna()
        print(f"\næ¸…ç†åæ•°æ®å½¢çŠ¶: {stocks_clean.shape}")
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(stocks_clean)
        print(f"âœ“ æ•°æ®æ ‡å‡†åŒ–å®Œæˆï¼Œå½¢çŠ¶: {scaled_data.shape}")
        
        # èšç±»åˆ†æ
        kmeans = KMeans(n_clusters=5, random_state=42)
        clusters = kmeans.fit_predict(scaled_data)
        print(f"âœ“ K-meansèšç±»å®Œæˆï¼Œå‘ç° {len(np.unique(clusters))} ä¸ªèšç±»")
        
        # åˆ†æèšç±»ç»“æœ
        stocks_clean['cluster'] = clusters
        cluster_stats = stocks_clean.groupby('cluster').mean()
        
        print("\nèšç±»ç»Ÿè®¡ç»“æœ:")
        print(cluster_stats)
        
        # èšç±»åˆ†å¸ƒ
        cluster_counts = pd.Series(clusters).value_counts().sort_index()
        print("\nå„èšç±»çš„æ ·æœ¬æ•°é‡:")
        print(cluster_counts)
        
        # å¯è§†åŒ–èšç±»ç»“æœ
        plt.figure(figsize=(15, 10))
        
        # 1. èšç±»åˆ†å¸ƒé¥¼å›¾
        plt.subplot(2, 3, 1)
        plt.pie(cluster_counts.values, labels=[f'èšç±»{i}' for i in cluster_counts.index], autopct='%1.1f%%')
        plt.title('èšç±»åˆ†å¸ƒ')
        
        # 2. èšç±»ä¸­å¿ƒçƒ­åŠ›å›¾
        plt.subplot(2, 3, 2)
        cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=stocks_clean.columns[:-1])
        sns.heatmap(cluster_centers, annot=True, cmap='RdYlBu_r', center=0)
        plt.title('èšç±»ä¸­å¿ƒçƒ­åŠ›å›¾')
        plt.ylabel('èšç±»')
        
        # 3. è‚¡ç¥¨æ”¶ç›Šç‡åˆ†å¸ƒ
        plt.subplot(2, 3, 3)
        for i in range(len(stocks_clean.columns)-1):
            plt.hist(stocks_clean.iloc[:, i], alpha=0.5, label=stocks_clean.columns[i], bins=30)
        plt.title('è‚¡ç¥¨æ”¶ç›Šç‡åˆ†å¸ƒ')
        plt.xlabel('æ”¶ç›Šç‡')
        plt.ylabel('é¢‘æ¬¡')
        plt.legend()
        
        # 4. èšç±»ç»“æœæ•£ç‚¹å›¾ï¼ˆå‰ä¸¤ä¸ªä¸»æˆåˆ†ï¼‰
        from sklearn.decomposition import PCA
        pca = PCA(n_components=2)
        pca_data = pca.fit_transform(scaled_data)
        
        plt.subplot(2, 3, 4)
        scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1], c=clusters, cmap='viridis')
        plt.colorbar(scatter)
        plt.title(f'PCAèšç±»å¯è§†åŒ–\n(è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%})')
        plt.xlabel('ç¬¬ä¸€ä¸»æˆåˆ†')
        plt.ylabel('ç¬¬äºŒä¸»æˆåˆ†')
        
        # 5. èšç±»å†…è‚¡ç¥¨æ”¶ç›Šç‡ç®±çº¿å›¾
        plt.subplot(2, 3, 5)
        cluster_returns = []
        cluster_labels = []
        for cluster_id in sorted(stocks_clean['cluster'].unique()):
            cluster_data = stocks_clean[stocks_clean['cluster'] == cluster_id]
            for col in cluster_data.columns[:-1]:  # æ’é™¤clusteråˆ—
                cluster_returns.extend(cluster_data[col].values)
                cluster_labels.extend([f'èšç±»{cluster_id}'] * len(cluster_data))
        
        cluster_df = pd.DataFrame({'æ”¶ç›Šç‡': cluster_returns, 'èšç±»': cluster_labels})
        sns.boxplot(data=cluster_df, x='èšç±»', y='æ”¶ç›Šç‡')
        plt.title('å„èšç±»æ”¶ç›Šç‡åˆ†å¸ƒ')
        plt.xticks(rotation=45)
        
        # 6. èšç±»ç¨³å®šæ€§åˆ†æ
        plt.subplot(2, 3, 6)
        inertias = []
        k_range = range(2, 11)
        for k in k_range:
            kmeans_temp = KMeans(n_clusters=k, random_state=42)
            kmeans_temp.fit(scaled_data)
            inertias.append(kmeans_temp.inertia_)
        
        plt.plot(k_range, inertias, 'bo-')
        plt.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='é€‰æ‹©çš„K=5')
        plt.title('è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜èšç±»æ•°')
        plt.xlabel('èšç±»æ•° (K)')
        plt.ylabel('ç°‡å†…å¹³æ–¹å’Œ (Inertia)')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig('clustering_analysis_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # è¯¦ç»†åˆ†ææŠ¥å‘Š
        print("\n" + "=" * 50)
        print("è¯¦ç»†èšç±»åˆ†ææŠ¥å‘Š")
        print("=" * 50)
        
        for cluster_id in sorted(stocks_clean['cluster'].unique()):
            cluster_data = stocks_clean[stocks_clean['cluster'] == cluster_id]
            print(f"\nèšç±» {cluster_id}:")
            print(f"  æ ·æœ¬æ•°é‡: {len(cluster_data)}")
            print(f"  å¹³å‡æ”¶ç›Šç‡: {cluster_data.iloc[:, :-1].mean().mean():.4f}")
            print(f"  æ”¶ç›Šç‡æ ‡å‡†å·®: {cluster_data.iloc[:, :-1].std().mean():.4f}")
            print(f"  æœ€ä½³è¡¨ç°è‚¡ç¥¨: {cluster_data.iloc[:, :-1].mean().idxmax()}")
            print(f"  æœ€å·®è¡¨ç°è‚¡ç¥¨: {cluster_data.iloc[:, :-1].mean().idxmin()}")
        
        # æŠ•èµ„å»ºè®®
        print("\n" + "=" * 50)
        print("åŸºäºèšç±»ç»“æœçš„æŠ•èµ„å»ºè®®")
        print("=" * 50)
        
        best_cluster = cluster_stats.mean(axis=1).idxmax()
        worst_cluster = cluster_stats.mean(axis=1).idxmin()
        
        print(f"\nğŸ¯ æ¨èèšç±»: èšç±» {best_cluster}")
        print(f"   - å¹³å‡æ”¶ç›Šç‡æœ€é«˜: {cluster_stats.mean(axis=1)[best_cluster]:.4f}")
        print(f"   - åŒ…å«è‚¡ç¥¨: {', '.join(cluster_stats.columns)}")
        
        print(f"\nâš ï¸  è°¨æ…èšç±»: èšç±» {worst_cluster}")
        print(f"   - å¹³å‡æ”¶ç›Šç‡æœ€ä½: {cluster_stats.mean(axis=1)[worst_cluster]:.4f}")
        
        # é£é™©åˆ†æ
        print(f"\nğŸ“Š é£é™©åˆ†æ:")
        volatilities = stocks_clean.groupby('cluster').std().mean(axis=1)
        print(f"   - æœ€ä½æ³¢åŠ¨èšç±»: èšç±» {volatilities.idxmin()} (æ ‡å‡†å·®: {volatilities.min():.4f})")
        print(f"   - æœ€é«˜æ³¢åŠ¨èšç±»: èšç±» {volatilities.idxmax()} (æ ‡å‡†å·®: {volatilities.max():.4f})")
        
        return True
        
    except FileNotFoundError:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° stock_returns.csv æ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œ generate_sample_data.py ç”Ÿæˆæ•°æ®æ–‡ä»¶")
        return False
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("è‚¡ç¥¨æ”¶ç›Šæ•°æ®èšç±»åˆ†ææµ‹è¯•")
    print("è¿™ä¸ªè„šæœ¬å°†æµ‹è¯•æ‚¨æä¾›çš„èšç±»åˆ†æä»£ç ç¤ºä¾‹")
    print()
    
    success = test_clustering_analysis()
    
    if success:
        print("\nâœ… èšç±»åˆ†ææµ‹è¯•å®Œæˆï¼")
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - clustering_analysis_results.png (å¯è§†åŒ–ç»“æœ)")
        print("\nğŸ’¡ æç¤º:")
        print("   - æ‚¨å¯ä»¥è°ƒæ•´èšç±»æ•°é‡ (n_clusters) æ¥è·å¾—ä¸åŒçš„èšç±»ç»“æœ")
        print("   - å¯ä»¥å°è¯•ä¸åŒçš„èšç±»ç®—æ³•ï¼Œå¦‚ DBSCAN æˆ–å±‚æ¬¡èšç±»")
        print("   - å»ºè®®ç»“åˆåŸºæœ¬é¢åˆ†ææ¥éªŒè¯èšç±»ç»“æœçš„åˆç†æ€§")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()